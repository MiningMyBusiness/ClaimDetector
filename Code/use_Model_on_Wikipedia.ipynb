{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run saved model on wikipedia page to classify text as needing citation\n",
    "\n",
    "Author: Kiran Bhattacharyya\n",
    "\n",
    "Revision: 5/11/18 - DRM - translate .py files into .ipynb, misc formatting\n",
    "\n",
    "1. loads in text from wikipedia page with python wikipedia library\n",
    "2. breaks text into sentences\n",
    "3. gets parts of speech and stems words to create binary feature vector\n",
    "4. tests each binary feature vector representing a sentence with the model\n",
    "5.  saves output in a text file where each line is a sentence with a 0 or 1 to indicate if the sentence needs citation or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import wikipedia\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create p_stemmer of class PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull text from wikipedia page\n",
    "Note: you can change the name if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPage = wikipedia.page(\"Perlin noise\") \n",
    "content = wikiPage.content # Content of page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract sentences from paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSents = nltk.sent_tokenize(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract words from sentences and find parts of speech of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsInSents = list()\n",
    "posInSents = list()\n",
    "for sent in fullSents:\n",
    "    sent_token = nltk.word_tokenize(sent) # word tokenize the sentence\n",
    "    sent_pos = nltk.pos_tag(sent_token) # tag with part of speech\n",
    "    stemWords = list() # create list to store stemmed words\n",
    "    onlyPOS = list() # create list to store sentence parts of speech\n",
    "    for item in sent_pos:\n",
    "        word = item[0]\n",
    "        stemWords.append(p_stemmer.stem(word.lower())) # lower case and stem words\n",
    "        onlyPOS.append(item[1])\n",
    "    wordsInSents.append(stemWords)\n",
    "    posInSents.append(onlyPOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### go through each word and the pos of each word to see if it matches the 600 feature names of the finalized model to create the 600 element binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames = np.load(\"../Data/FeatureName_finalized_model.npy\")\n",
    "featureMat = np.zeros((len(wordsInSents), len(featureNames)))\n",
    "for i in range(0,len(wordsInSents)):\n",
    "    words = wordsInSents[i]\n",
    "    POSes = posInSents[i]\n",
    "    for j in range(0,len(featureNames)):\n",
    "        name = featureNames[j]\n",
    "        isWord = name in words\n",
    "        isPOS = name in POSes\n",
    "        if isWord or isPOS:\n",
    "            featureMat[i,j] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feed the binary vector into the model to get a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"../Data/finalized_model.sav\", \"rb\"))\n",
    "myPredictions = loaded_model.predict(featureMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write out each sentence and its prediction into a new text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open(\"../Data/classifiedText_wikipedia.txt\", \"w\")\n",
    "\n",
    "for i in range(0,len(fullSents)):\n",
    "    sent = fullSents[i]\n",
    "    sent = sent.encode(\"ascii\", \"ignore\")\n",
    "    sent_complete = sent + \"    \" + str(myPredictions[i]) + \"\\n\"\n",
    "    file.write(sent_complete)\n",
    "\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
