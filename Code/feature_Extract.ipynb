{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from clauses and sentences that need citations and those that do not\n",
    "\n",
    "Author: Kiran Bhattacharyya\n",
    "\n",
    "Revision: 5/11/18 - DRM - translate .py files into .ipynb, misc formatting \n",
    "\n",
    "\n",
    "this code reads in two data files:\n",
    "1. one contains sentences and clauses that need citations\n",
    "2. the other contains sentences that do no\n",
    "\n",
    "Then it filters the words in the sentence by parts of speech, and stems the words\n",
    "\n",
    "It also calculates the occurance of the unique words and parts of speech in the two datasets\n",
    "\n",
    "finally it saves these filtered data sets and the counts of the unique features in each dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create p_stemmer object\n",
    "p_stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data which contain sentences that need citations and sentences that do not (are not claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "needCite = pd.read_pickle('../Data/CitationNeeded.pkl') # need citations\n",
    "noClaim = pd.read_pickle('../Data/NotACLaim.pkl') # do NOT need citations (are not claims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize sentences into words and tag parts of speech\n",
    " keep nouns (NN), adjectives (JJ), verbs (VB), adverbs (RB), numberical/cardinal (CD), determiner (DT)\n",
    "\n",
    "features will include words that are any of the previous, the length of the sentence or clause\n",
    "\n",
    "First for claim data: \n",
    "\n",
    "5/16/18 DRM - removed .encode from thisWord to allow python 3 compatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "needCite_filtSent = list() # list to store word tokenized and filtered sentences from citation needed list\n",
    "needCite_wordTag = list() # list to store the part of speech of each word\n",
    "noClaim_filtSent = list() # list to store word tokenized and filtered sentences from not a claim list\n",
    "noClaim_wordTag = list() # list to store the part of speech of each word\n",
    "allWordList = list() # list that stores all words in both data sentences\n",
    "allPOSList = list() # list that stores all POS of all words in both datasets\n",
    "\n",
    "for sent in needCite.CitationNeeded:\n",
    "    sent_token = nltk.word_tokenize(sent) # word tokenize the sentence\n",
    "    sent_pos = nltk.pos_tag(sent_token) # tag with part of speech\n",
    "    sent_filt_word = list() # create list to store filtered sentence words\n",
    "    sent_filt_pos = list() # create list to store the filtered parts of speech\n",
    "    for item in sent_pos: # for each item in the sentence\n",
    "        if len(item) > 1:\n",
    "            thisTag = item[1] # grab the part of speech\n",
    "            if 'NN' in thisTag or 'JJ' in thisTag or 'VB' in thisTag or 'RB' in thisTag or 'CD' in thisTag or 'DT' in thisTag: # if the tag is an approved part of speech\n",
    "                thisWord = item[0]\n",
    "                sent_filt_word.append(p_stemmer.stem(thisWord.lower()))\n",
    "                sent_filt_pos.append(thisTag)\n",
    "                allWordList.append(p_stemmer.stem(thisWord.lower()))\n",
    "                allPOSList.append(thisTag)\n",
    "    needCite_filtSent.append(sent_filt_word)\n",
    "    needCite_wordTag.append(sent_filt_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wuxia',\n",
       "  'work',\n",
       "  'were',\n",
       "  'deem',\n",
       "  'respons',\n",
       "  'brew',\n",
       "  'anti-govern',\n",
       "  'sentiment',\n",
       "  'which',\n",
       "  'led',\n",
       "  'rebellion',\n",
       "  'those',\n",
       "  'era'],\n",
       " ['see',\n",
       "  'a',\n",
       "  'parent',\n",
       "  'beat',\n",
       "  'child',\n",
       "  'observ',\n",
       "  'such',\n",
       "  'an',\n",
       "  'action',\n",
       "  'is',\n",
       "  'either',\n",
       "  'good',\n",
       "  'bad']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needCite_filtSent[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NNP',\n",
       "  'NNS',\n",
       "  'VBD',\n",
       "  'VBN',\n",
       "  'JJ',\n",
       "  'VBG',\n",
       "  'JJ',\n",
       "  'NNS',\n",
       "  'WDT',\n",
       "  'VBD',\n",
       "  'NNS',\n",
       "  'DT',\n",
       "  'NNS'],\n",
       " ['VBP',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VB',\n",
       "  'NN',\n",
       "  'VBP',\n",
       "  'PDT',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'RB',\n",
       "  'JJ',\n",
       "  'JJ']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needCite_wordTag[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(needCite_filtSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in noClaim.NotAClaim:\n",
    "    sent_token = nltk.word_tokenize(sent) # word tokenize the sentence\n",
    "    sent_pos = nltk.pos_tag(sent_token) # tag with part of speech\n",
    "    sent_filt_word = list() # create list to store filtered sentence words\n",
    "    sent_filt_pos = list() # create list to store the filtered parts of speech\n",
    "    for item in sent_pos: # for each item in the sentence\n",
    "        if len(item) > 1:\n",
    "            thisTag = item[1] # grab the part of speech\n",
    "            if 'NN' in thisTag or 'JJ' in thisTag or 'VB' in thisTag or 'RB' in thisTag or 'CD' in thisTag or 'DT' in thisTag: # if the tag is an approved part of speech\n",
    "                thisWord = item[0]\n",
    "                sent_filt_word.append(p_stemmer.stem(thisWord.lower()))\n",
    "                sent_filt_pos.append(thisTag)\n",
    "                allWordList.append(p_stemmer.stem(thisWord.lower()))\n",
    "                allPOSList.append(thisTag)\n",
    "    noClaim_filtSent.append(sent_filt_word)\n",
    "    noClaim_wordTag.append(sent_filt_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['observ',\n",
       "  'philosoph',\n",
       "  'term',\n",
       "  'is',\n",
       "  'the',\n",
       "  'process',\n",
       "  'filter',\n",
       "  'sensori',\n",
       "  'inform',\n",
       "  'the',\n",
       "  'thought',\n",
       "  'process'],\n",
       " ['input',\n",
       "  'is',\n",
       "  'receiv',\n",
       "  'hear',\n",
       "  'sight',\n",
       "  'smell',\n",
       "  'tast',\n",
       "  'touch',\n",
       "  'then',\n",
       "  'analyz',\n",
       "  'either',\n",
       "  'ration',\n",
       "  'irrat',\n",
       "  'thought']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noClaim_filtSent[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a,b):\n",
    "    return a,b,a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 12\n"
     ]
    }
   ],
   "source": [
    "[a,b,c]=test(3,4)\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute word occurances in sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "uniqWords = list(set(allWordList)) # find all uniqwords in the dataset\n",
    "wordOccur_claim  = list() # list to store number of times word occurs in claim dataset\n",
    "wordOccur_notClaim = list() # list to store number of times word occurs in not claim data set\n",
    "\n",
    "def unique(uniq,needCite_filtSent,noClaim_filtSent,claim,notClaim):\n",
    "        word = uniq[i]\n",
    "        numOfTimes = 0\n",
    "        for sent in needCite_filtSent:\n",
    "            if word in sent:\n",
    "                numOfTimes = numOfTimes + len([j for j, x in enumerate(sent) if x == word])\n",
    "        claim.append(numOfTimes)\n",
    "        numOfTimes = 0\n",
    "        for sent in noClaim_filtSent:\n",
    "            if word in sent:\n",
    "                numOfTimes = numOfTimes + len([j for j, x in enumerate(sent) if x == word])\n",
    "        notClaim.append(numOfTimes)\n",
    "    return claim, notClaim\n",
    "\n",
    "# [wordOccur_claim,wordOccur_notClaim]=unique(uniqWords,needCite_filtSent,noClaim_filtSent,\n",
    "#                   wordOccur_claim,wordOccur_notClaim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start @ 6:05pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wordOccur_claim,wordOccur_notClaim]=Parallel(n_jobs=-2)(delayed(unique)(uniq=uniqWords,needCite_filtSent=needCite_filtSent,\n",
    "                                    noClaim_filtSent=noClaim_filtSent,\n",
    "                                    claim=wordOccur_claim,\n",
    "                                    notClaim=wordOccur_notClaim)  for i in range(0,len(uniqWords)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute POS occurances in sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqPOS = list(set(allPOSList)) # find all uniqwords in the dataset\n",
    "posOccur_claim = list() # for part of speech\n",
    "posOccur_notClaim = list()\n",
    "\n",
    "for i in range(0,len(uniqPOS)): # for each word\n",
    "    word = uniqPOS[i]\n",
    "    numOfTimes = 0\n",
    "    for sent in needCite_wordTag:\n",
    "        if word in sent:\n",
    "            numOfTimes = numOfTimes + len([j for j, x in enumerate(sent) if x == word])\n",
    "    posOccur_claim.append(numOfTimes)\n",
    "    numOfTimes = 0\n",
    "    for sent in noClaim_wordTag:\n",
    "        if word in sent:\n",
    "            numOfTimes = numOfTimes + len([j for j, x in enumerate(sent) if x == word])\n",
    "    posOccur_notClaim.append(numOfTimes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqWords = pd.DataFrame(\n",
    "    {'UniqueWords': uniqWords,\n",
    "    'WordOccurClaim': wordOccur_claim,\n",
    "    'WordOccurNotClaim': wordOccur_notClaim\n",
    "    })\n",
    "UniqWords.to_pickle('../Data/UniqueWords.pkl')\n",
    "\n",
    "UniqPOS = pd.DataFrame(\n",
    "    {'UniquePOS': uniqPOS,\n",
    "    'POSOccurClaim': posOccur_claim,\n",
    "    'POSOccurNotClaim': posOccur_notClaim\n",
    "    })\n",
    "UniqPOS.to_pickle('../Data/UniquePOS.pkl')\n",
    "\n",
    "NeedCite = pd.DataFrame(\n",
    "    {'NeedCiteWord': needCite_filtSent,\n",
    "    'NeedCitePOS': needCite_wordTag\n",
    "    })\n",
    "NeedCite.to_pickle('../Data/NeedCiteFilt.pkl')\n",
    "\n",
    "NotClaim = pd.DataFrame(\n",
    "    {'NotClaimWord': noClaim_filtSent,\n",
    "    'NotClaimPOS': noClaim_wordTag\n",
    "    })\n",
    "NotClaim.to_pickle('../Data/NotClaimFilt.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
